---
layout: page
title: Publications
subtitle: Journals as well as Conference Proceedings.
permalink: /publications/
feature-img: "assets/img/onsite/forPubli.png"
tags: [publications, journal, conference]
---

## Journal Publications

1. Dvoretskii, S., Gong, Z., **Gupta, A.**, Parent, J., & Alicea, B. (2022). Braitenberg vehicles as developmental neurosimulation. *Artificial Life (MIT Press)*, *28*(3), 369–395.
<br>  
[DOI](https://doi.org/10.1162/artl_a_00384) | [PDF](https://scholar.google.com/scholar_url?url=https://direct.mit.edu/artl/article-pdf/28/3/369/2037990/artl_a_00384.pdf%3Fcasa_token%3D1fURgGQGKKQAAAAA:BzkGHADEBCCo5yl8C41oI2b5auYptJGZq7xcu1b9KFpKpWzAR40gFPBNcpc_-EPB8BszSv705cI&hl=en&sa=T&oi=ucasa&ct=ucasa&ei=XBIoZ6wn8MjL1g_TwsrxBQ&scisig=AFWwaeYSNXGh0U3HIMDxv6Cul-zc)
<br>    
**Abstract:** Connecting brain and behavior is a longstanding
issue in the areas of behavioral science, artificial intelligence, and
neurobiology. As is standard among models of artificial and
biological neural networks, an analogue of the fully mature brain is
presented as a blank slate. However, this does not consider the
realities of biological development and developmental learning.
Our purpose is to model the development of an artificial organism
that exhibits complex behaviors. We introduce three alternate
approaches to demonstrate how developmental embodied agents
can be implemented. The resulting developmental Braitenberg
vehicles (dBVs) will generate behaviors ranging from stimulus
responses to group behavior that resembles collective motion.
We will situate this work in the domain of artificial brain networks
along with broader themes such as embodied cognition, feedback,
and emergence. Our perspective is exemplified by three software
instantiations that demonstrate how a BV-genetic algorithm hybrid
model, a multisensory Hebbian learning model, and multi-agent
approaches can be used to approach BV development. We introduce
use cases such as optimized spatial cognition (vehicle-genetic
algorithm hybrid model), hinges connecting behavioral and neural
models (multisensory Hebbian learning model), and cumulative
classification (multi-agent approaches). In conclusion, we consider
future applications of the developmental neurosimulation approach.
<br>    
**AI-Generated Paper-Discussion Podcast (by NotebookLM):**\
     <br>
     <audio controls>
       <source src="https://raw.githubusercontent.com/ankiitgupta7/ankiitgupta7.github.io/master/assets/audio/Alife_BV.wav" type="audio/wav">
       Your browser does not support the audio element.
     </audio>
<br>  
2. Ke, S.-C., **Gupta, A.**, Lo, Y.-H., Ting, C.-C., & Tseng, P. (2023). The hidden arrow in the FedEx logo: Do we really unconsciously “see” it? *Cognitive Research: Principles and Implications (Springer International Publishing)*, *8*(1), 40.
<br>  
[DOI](https://doi.org/10.1186/s41235-023-00494-x) | [PDF](https://cognitiveresearchjournal.springeropen.com/counter/pdf/10.1186/s41235-023-00494-x.pdf)
<br>    
**Abstract:** The FedEx logo makes clever use of fgure-ground ambiguity to create an “invisible” arrow in the background space 
between “E” and “x”. Most designers believe the hidden arrow can convey an unconscious impression of speed and 
precision about the FedEx brand, which may infuence subsequent behavior. To test this assumption, we designed 
similar images with hidden arrows to serve as endogenous (but camoufaged) directional cues in a Posner’s orient‑
ing task, where a cueing efect would suggest subliminal processing of the hidden arrow. Overall, we observed no 
cue congruency efect, unless the arrow is explicitly highlighted (Experiment 4). However, there was a general efect 
of prior knowledge: when people were under pressure to suppress background information, those who knew about 
the arrow could do so faster in all congruence conditions (i.e., neutral, congruent, incongruent), although they fail to 
report seeing the arrow during the experiment. This was true in participants from North America who had heard of 
the FedEx arrow before (Experiment 1 & 3), and also in our Taiwanese sample who were just informed of such design 
(Experiment 2). These results can be well explained by the Biased Competition Model in fgure-ground research, and 
together suggest: (1) people do not unconsciously perceive the FedEx arrow, at least not enough to exhibit a cueing 
efect in attention, but (2) knowing about the arrow can fundamentally change the way we visually process these 
negative-space logos in the future, making people react faster to images with negative space regardless of the hid‑
den content.  
<br>    
**AI-Generated Paper-Discussion Podcast (by NotebookLM):**  
     <br>
     <audio controls>
       <source src="https://raw.githubusercontent.com/ankiitgupta7/ankiitgupta7.github.io/master/assets/audio/FedEx_Paper.wav" type="audio/wav">
       Your browser does not support the audio element.
     </audio>
<br>  

3. **Gupta, A.**, Lo, Y.-H., Cheng, T., & Tseng, P. (2024). The sustenance & retention of perspectival shape representations. *Consciousness and Cognition*, *126*, 103788.
<br>  
[DOI](hhttps://doi.org/10.1016/j.concog.2024.103788) | [PDF](https://www.sciencedirect.com/science/article/abs/pii/S1053810024001557?via%3Dihub)
<br>    
**Abstract:** When we are presented with a coin rotated in depth, although we perceive its objective circular 
shape, the original perspectival shape is nonetheless represented in the visual system. Here we 
investigated the onset time and duration of such perspectival representation by systematically 
manipulating stimuli presentation time vs. post-stimuli retention time. Participants performed a 
speeded search task and had to find an oval target against a circle distractor that is either head-on 
(i.e., perspectivally dissimilar) or rotated leftward/rightward (i.e., perspectivally similar). We 
found that even when stimuli disappeared from view, participants still took more time in locating 
the oval target, suggesting robust and persistent perspectival interference (Experiment 1). This 
interference emerged as early as 100 ms (Experiment 2), and persisted for at least 1000 ms 
(Experiment 3). Together, these results suggest a 100 ms formation time and possibly 1000 ms or 
longer life span for perspectival representation. 
<br>    
**AI-Generated Paper-Discussion Podcast (by NotebookLM):**  
     <br>
     <audio controls>
       <source src="https://raw.githubusercontent.com/ankiitgupta7/ankiitgupta7.github.io/master/assets/audio/Perspectival Shape Representation.wav" type="audio/wav">
       Your browser does not support the audio element.
     </audio>
---

## Conference Proceedings

1. **Gupta, A.**, & Steen, F. (2024). Simulating representational communication in vervet monkeys using agent-based simulation. In *Proceedings of the 15th International Conference on the Evolution of Language* (pp. 229–231). Max Planck Institute for Psycholinguistics.
<br>   
[DOI](https://doi.org/10.17617/2.3587960) | [PDF](https://pure.mpg.de/rest/items/item_3587960_5/component/file_3600212/content)
<br>     
**Introduction:** Human languages are composed in part by discrete patterns of sound that can be linked to meanings through social learning. What are the evolutionary pressures that drove the development of referential vocalization? In this project, we explore the survival costs and benefits of referential alarm calls in a savanna-dwelling primate. We base our model on the discovery that the African vervet monkey (Chlorocebus pygerythrus) gives acoustically different alarm calls to different predators (Struhsaker, 1967; Seyfarth & Cheney, 1980a, b, c), conditional on context (Deshpande et al., 2023). In the following, we simulate the costs that vervets incur by monitoring their environment for predators, issuing alarm calls, running to escape potential predators, and foregoing foragingin favor of seeking refuge, and track the survival outcomes. Our goal is to determine the envelope of evolvability of representational signaling in the parameter space of a troop of primates in an African savannah environment. 
<br>     
**AI-Generated Paper-Discussion Podcast (by NotebookLM):**\
     <br>   
     <audio controls>
       <source src="https://raw.githubusercontent.com/ankiitgupta7/ankiitgupta7.github.io/master/assets/audio/vervetSim.wav" type="audio/wav">
       Your browser does not support the audio element.
     </audio>  

---
\
For more information, please refer to my [Google Scholar](https://scholar.google.com/citations?user=FTCbGjoAAAAJ&hl=en).